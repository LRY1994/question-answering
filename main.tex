
\documentclass[sigconf]{acmart}
\usepackage{graphicx}
\usepackage{enumerate}

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
	\providecommand\BibTeX{{%
			\normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmcopyright}
\copyrightyear{2018}
\acmYear{2018}
\acmDOI{10.1145/1122445.1122456}

%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[Woodstock '18]{Woodstock '18: ACM Symposium on Neural
	Gaze Detection}{June 03--05, 2018}{Woodstock, NY}
\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
	June 03--05, 2018, Woodstock, NY}
\acmPrice{15.00}
\acmISBN{978-1-4503-XXXX-X/18/06}


\begin{document}
	
	\title{Improving open domain question answering with Knowledge Base and Wikipedia graph}
	
	\author{Ruiyu Lin}
	\affiliation{%
		\institution{SUN YAT-SEN UNIVERSITY}}
	\email{linry23@mail2.sysu.edu.cn}
	
	
	\begin{abstract}
		A clear and well-documented \LaTeX\ document is presented as an
		
	\end{abstract}

	\keywords{ neural networks}

	\maketitle
	
	\section{Introduction}
		Open-domain  Question Answering mostly focus on factoid question answering,which require systems to return a short and concise answer to these questions.
		Most existing models, however,answer questions using a single information
		source, usually either text from an text corpus such as Wikipedia\cite{chen2017reading},or a single knowledge base (KB).	
		
		Large-scale factual knowledge bases such as WikiData and Freebase\cite{bollacker2008freebase}, stores a large number of facts in an organized way. Namely, Freebase has 46m entities and 2.6b facts, WikiData contains 87m items .Each fact is made of two entities and a relation between them. Most konwledge bases are curated,ensureing the correctness of the information,common or "simple" questions can be answered easily if semantic parsing (question query)  is done correctly.The advantage of graph structure also enables multi-hop question answering.
		Unfortunately, curated konwledge bases, which demands tremendous hunman labor,might not keep up with times,thereby some relations would be missing.Limited coverage of questions  can be answered because the resoning is based on the similarity over relationships and entities.	
		
		Wikipedia\cite{chen2017reading}, a text source, was proposed for the first time to process Open Domain QA tasks, and a DRQA system was developed, including Document Retriever and Document Reader, which laid the pipe-line, two-stage approaches, of QA for successive work.We also follow this tradition ,retrieve and then read. 		
		Text corpus provides a more completed coverage of facts,and it is easy to catch the time,however lacks the ability of multi-hop resoning.
		
		To combine the coverage of text evidence and reasoning ability of knowledge base, some recent work  use both text and KB,to constructs graphs of nodes and edges\cite{sun2018open,sun2019pullnet,xiong2019improving}.These works basically augment the KB graph with the entities indentified from the relevent text evidences,the task of answer determination is then reduced to classfiy the entity node is the answer or not.
		Another line\cite{min2019knowledge} ,inversely,augment retrieval passage with KB graph, and the task of answer determination is to do answer extraction from text.
		
	\section{Related work}
	\subsection{QA using Text}
	
	\subsection{QA using KB}
	\subsection{QA using both Text and KB}
	In {\itshape GRAFT-Net}\cite{sun2018open} , {\itshape PullNet}\cite{sun2019pullnet},and{ \itshape Knowledge-Aware}\cite{xiong2019improving},the  answers is restricted to be the KB entities. Meanwhile, their question subgraph is heterogeneous, which contains   KB triples(entity,relation,entity), and entity-linked text. The task of QA then reduces to learning the representations of the nodes, and then performing a binary classification over these nodes to decide whether it is the answer or not. They both augmented knowledge bases with text from Wikipedia, which means KB dominates the whole process.
	
	However,{\itshape Knowledge guided}\cite{min2019knowledge} construct the graph in a different way.  Inversely, the knowledge base is used to better model relationships between different passages of text, which means the text corpus dominates instead. Its question subgraph is not heterogeneous, only contains entity-linked passage and relations. The task of QA  switches to learning the representations of the passage . Not to classify the node is the answer or not, it extracts the most possible span as answer in the most possible passage as prior work did.Our work is consistent with it.
	
	\section{Method}
	
	\textbf{Input}
		\begin{itemize}
			\item {Dataset}:(question, answer) pair.
			
			\item{ Knowledge Base}:(entity, relation, entity) triple.
			 Knowledge Base is a multi-relational graphs, each edge has a label and direction associated with it, and each node in the graph is an entity.
			 
			 \item{ Wikipedia graph\cite{asai2019learning}}:(passage, passage) pair.
			 Wikipedia graph is constructed by hyperlinks and within-document links,the edge is direct ,and  each node in the graph is an article.The Wikipedia graph is densely connected and covers a wide range of topics that provide useful evidence for open-domain questions
				
		\end{itemize}
		

	\textbf{Output}
	representation of all the retrieved passage, as the input to a reader model to extract answer.
	
	\textbf{Goal} 
	To better embed the retrieved passage, which based on Wikipedia graph, with  Knowledge Base knowledge.
	
	\textbf{Method} 
	Fuse Knowledge Base knowledge into Wikipedia graph to formulate
	(passage, relation, passage) triple.
	\begin{enumerate}[(1)]
		\item get the seed passage by a TF-IDF based retrieval system.Around the seed passage,take the neighbouring passage from Wikipedia graph,$(P_1,...,P_N)$
		
		\item identify the relations between each two passage node
		
		assuming that :
				
		$rSet_{(a,b)}$ : the set of relation between $\MakeUppercase{p}_a$ and $\MakeUppercase{p}_b$, initialized empty
		
		($\MakeUppercase{p}_a$, 	$\MakeUppercase{p}_b$) exits in Wikipedia graph.
		
		
		$\MakeUppercase{p}_a$ contains n entities( $e_{a1},...,e_{an}$) ,
		
		$\MakeUppercase{p}_b$ contains m entities( $e_{b1},...,e_{bm}$) 
		
		
		If $(e_{ai}, r, e_{bj})$(1<=i,j<=n) exits in Knowledge Base, add r into $rSet_{(a,b)}$ . Finally, $rSet_{a,b} = (r_{1}...,r_{k}) $
		
		\item corperate identified relations into passage node embedding.	Suppose that  $h_{b}$ is link by  $[h_{a1},...,h_{at}]$ ,then update $h_{b}$ by 
		\begin{displaymath}
			\begin{aligned}
			&\alpha_{r_i} = sorce( h_{a_i},e_{ai},r_i,q) \\		
			&h_{b}= GCN(h_{b}, \sum_{1}^{t}FNN (h_{a_i}, \sum_{i=1}^{k} \alpha_{r_i}*ri))
			\end{aligned}
		\end{displaymath}
		
		$\alpha_{r_i}$ is the relation score.
	

	
	\item answer extraction
		
	Denote the passage score as $Pr(P_i|Q, P)$,which reranks all retrieved passages
		\begin{displaymath}
			Pr(P_i|Q, P) = softmax(h_i^\mathrm{ T }  W)
		\end{displaymath}
			
	W is a trainable parameter.
	
	The score of an answer span from passage $\MakeUppercase{p}_i$ will be
			\begin{displaymath}
			Pr(a| Q, P) = 	Pr(P_i|Q, P)P_s(a_s|Q, P)P_e(a_e|Q, P).
			\end{displaymath}
	
		
		
		
	\end{enumerate} 
   
   	\textbf{Some details remains to be determined}
   	
   	\begin{enumerate}[(1)]
   		
   	\item how to get the representation of relation.
   	
	   	In GRAFT-Nets\cite{sun2018open}, they average word
	   	vectors to compute a relation vector  from the
	   	surface form of the relation.
	   	
	   	In PullNet\cite{sun2019pullnet} ,embedding of  relations are pretrained ,and can be looked up from an embedding table.
	   	
	   	In \cite{min2019knowledge} ,they only consider the most frequent 100 relations, and pretrain their embedding.	
	   	
	   	 \cite{xiong2019improving} tokenize the relation ,and then  encode it by a shared LSTM with the question .
	   
	   	
	   	In \cite{2020Composition},it handles multi-relational graphs  representation where each edge has a label and direction associated with it, and jointly embeds both nodes and relations in a relational graph.
	   	
	   	\begin{figure*}[ht]		
	   		\centering
	   		\includegraphics[scale=0.5]{f1.jpg}
	   		\caption{A diagram of approach}
	   		\label{fig:label}
	   	\end{figure*}
	   	
   	
   	\item the representaion of question
   	
   		In GRAFT-Nets\cite{sun2018open},
   		the question representation is updated as
   		$h_q = FFN(\sum_{v\in S_q}h_v)$
   		, where $S_q$ denotes the seed entities mentioned in the question.
   		
   		In PullNet\cite{sun2019pullnet}and \cite{xiong2019improving} ,	the question representation is accquired by a LSTM.
   		
   		In \cite{min2019knowledge} ,the question is not directly encoded,but with passage jointly by BERT.
   		
   		
   	
   	
   	\item how to score relation attention
   	
	   	Most work take the dot product between the embedding of relation and question.Considering the different framework here,we reformulate the score as:
	   	 
	   	 \begin{displaymath}
	   	 	\begin{aligned}
	   	 		&\alpha_{r_i} = sorce( h_{a},e_{ai},r_i,q)    	 		
	   	 	\end{aligned}
	   	 \end{displaymath}
	     	
   \end{enumerate} 


	
	%%
	%% The next two lines define the bibliography style to be used, and
	%% the bibliography file.
	\bibliographystyle{ACM-Reference-Format}
	\bibliography{refs}
	
	
	
\end{document}
\endinput
%%
%% End of file `sample-sigconf.tex'.
